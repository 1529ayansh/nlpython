{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis Analysis Part 3— Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of a support vector machine classifier for the Sentiment analysis problem. <br>\n",
    "For explanation of the code, visit https://medium.com/nlpython/sentiment-analysis-analysis-part-2-support-vector-machines-31f78baeee09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"reviews.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"reviews.txt\") as f:\n",
    "    reviews = f.read().split(\"\\n\")\n",
    "with open(\"labels.txt\") as f:\n",
    "    labels = f.read().split(\"\\n\")\n",
    "    \n",
    "reviews_tokens = [review.split() for review in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelBinarizer(classes=None, sparse_output=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "onehot_enc = MultiLabelBinarizer()\n",
    "onehot_enc.fit(reviews_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews_tokens, labels, test_size=0.4, random_state=None)\n",
    "\n",
    "split_point = int(len(X_test)/2)\n",
    "X_valid, y_valid = X_test[split_point:], y_test[split_point:]\n",
    "X_test, y_test = X_test[:split_point], y_test[:split_point]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "vocab_len = len(onehot_enc.classes_)\n",
    "inputs_ = tf.placeholder(dtype=tf.float32, shape=[None, vocab_len], name=\"inputs\")\n",
    "targets_ = tf.placeholder(dtype=tf.float32, shape=[None, 2], name=\"targets\")\n",
    "\n",
    "h1 = tf.layers.dense(inputs_, 500, activation=tf.nn.relu)\n",
    "#h2 = tf.layers.dense(h1, 500, activation=tf.nn.relu)\n",
    "logits = tf.layers.dense(h1, 2, activation=None)\n",
    "output = tf.nn.sigmoid(logits)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=targets_))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(targets_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label2bool(labels):\n",
    "    return [[1,0] if label == \"positive\" else [0,1] for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(X, y, batch_size):\n",
    "    for batch_pos in range(0,len(X),batch_size):\n",
    "        yield X[batch_pos:batch_pos+batch_size], y[batch_pos:batch_pos+batch_size]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Training loss: 0.6979156732559204\n",
      "Epoch: 0 \t Validation Accuracy: 0.5709999799728394\n",
      "Epoch: 1 \t Training loss: 0.5234161019325256\n",
      "Epoch: 1 \t Validation Accuracy: 0.7129999995231628\n",
      "Epoch: 2 \t Training loss: 0.4143674373626709\n",
      "Epoch: 2 \t Validation Accuracy: 0.8370000123977661\n",
      "Epoch: 3 \t Training loss: 0.27750399708747864\n",
      "Epoch: 3 \t Validation Accuracy: 0.8650000095367432\n",
      "Epoch: 4 \t Training loss: 0.20388154685497284\n",
      "Epoch: 4 \t Validation Accuracy: 0.8740000128746033\n",
      "Epoch: 5 \t Training loss: 0.1524941474199295\n",
      "Epoch: 5 \t Validation Accuracy: 0.8840000033378601\n",
      "Epoch: 6 \t Training loss: 0.10476348549127579\n",
      "Epoch: 6 \t Validation Accuracy: 0.8640000224113464\n",
      "Epoch: 7 \t Training loss: 0.08236084133386612\n",
      "Epoch: 7 \t Validation Accuracy: 0.859000027179718\n",
      "Epoch: 8 \t Training loss: 0.06472384929656982\n",
      "Epoch: 8 \t Validation Accuracy: 0.8820000290870667\n",
      "Epoch: 9 \t Training loss: 0.04698138311505318\n",
      "Epoch: 9 \t Validation Accuracy: 0.8930000066757202\n",
      "Test Accuracy: 0.902999997138977\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 3000\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initializing the variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(epochs):\n",
    "    for X_batch, y_batch in get_batch(onehot_enc.transform(X_train), label2bool(y_train), batch_size):         \n",
    "        loss_value, _ = sess.run([loss, optimizer], feed_dict={\n",
    "            inputs_: X_batch,\n",
    "            targets_: y_batch\n",
    "        })\n",
    "        print(\"Epoch: {} \\t Training loss: {}\".format(epoch, loss_value))\n",
    "\n",
    "    acc = sess.run(accuracy, feed_dict={\n",
    "        inputs_: onehot_enc.transform(X_valid),\n",
    "        targets_: label2bool(y_valid)\n",
    "    })\n",
    "\n",
    "    print(\"Epoch: {} \\t Validation Accuracy: {}\".format(epoch, acc))\n",
    "\n",
    "test_acc = sess.run(accuracy, feed_dict={\n",
    "    inputs_: onehot_enc.transform(X_test),\n",
    "    targets_: label2bool(y_test)\n",
    "})\n",
    "print(\"Test Accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "#Test model\n",
    "sentence = \"great movie\"\n",
    "sentence_tokens = onehot_enc.transform([sentence.split()])\n",
    "\n",
    "result = sess.run(output, feed_dict={\n",
    "    inputs_: sentence_tokens\n",
    "})\n",
    "if result[0][0] > result[0][1]:\n",
    "    print(\"positive\")\n",
    "else:\n",
    "    print(\"negative\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
